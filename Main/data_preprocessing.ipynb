{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cdenq/bacteria-diversity-interactive-web-dashboard/blob/main/Main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrdjOVkdtub4",
        "outputId": "3a957569-658c-4395-bde8-5ae010121e32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('drive/My Drive/bootcamp_colab/Resources/secondary_data.csv', sep=';')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "VmzA3e3Yu9b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "jVpReCyZu9jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_smaller = df.drop(columns={'stem-root', 'stem-surface','veil-type', 'veil-color', 'spore-print-color', 'gill-spacing'})\n",
        "df_smaller.head()"
      ],
      "metadata": {
        "id": "u2H6wv2Ru9mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropped columns that had large number of nan\n",
        "#nan could be a useful classifier, consider re-running with the nans\n"
      ],
      "metadata": {
        "id": "ls0gDsDrzLPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping rows with nans\n",
        "df_smaller.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "SphWkEmvzLkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one-hot encoding\n",
        "df_dummies = pd.get_dummies(df_smaller)"
      ],
      "metadata": {
        "id": "4jPDm9YgwY_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dummies"
      ],
      "metadata": {
        "id": "G4iXkl5R0cFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_dummies.drop(columns = {'class_e','class_p'})\n",
        "y = df_dummies['class_e']"
      ],
      "metadata": {
        "id": "eNyn1_JP0cQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use sklearn to split dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "metadata": {
        "id": "1jlcP-dCw6ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#scale\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X_scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler\n",
        "X_scaler.fit(X_train)\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = X_scaler.transform(X_train)\n",
        "X_test_scaled = X_scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "LLjuq8IAyELn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def rfc(X_train, X_test, y_train, y_test):\n",
        "  clf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "  clf.fit(X_train, y_train)\n",
        "  score = clf.score(X_test, y_test)\n",
        "  y_pred_Log = clf.predict(X_test)\n",
        "  print(classification_report(y_test, y_pred_Log))\n",
        "  # clf.predict(X_test)\n",
        "  return score"
      ],
      "metadata": {
        "id": "L8cqVa51yENo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc(X_train_scaled, X_test_scaled, y_train, y_test)"
      ],
      "metadata": {
        "id": "vSz4wi7J7uZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf12b4a-9b3b-4f18-c9bc-75ee42ccfdc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      5069\n",
            "           1       1.00      1.00      1.00      4198\n",
            "\n",
            "    accuracy                           1.00      9267\n",
            "   macro avg       1.00      1.00      1.00      9267\n",
            "weighted avg       1.00      1.00      1.00      9267\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def lr(X_train, X_test, y_train, y_test):\n",
        "  model = LogisticRegression()\n",
        "  model.fit(X_train, y_train)\n",
        "  score = model.score(X_test, y_test)\n",
        "  y_pred_Log = model.predict(X_test)\n",
        "  print(classification_report(y_test, y_pred_Log))\n",
        "  return score"
      ],
      "metadata": {
        "id": "qSK2-dx4-pyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr(X_train_scaled, X_test_scaled, y_train, y_test)"
      ],
      "metadata": {
        "id": "UVFPVpy6_zqn",
        "outputId": "a15ee52a-66e4-4c04-d01b-4173162f4ad0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.79      0.81      5069\n",
            "           1       0.76      0.79      0.78      4198\n",
            "\n",
            "    accuracy                           0.79      9267\n",
            "   macro avg       0.79      0.79      0.79      9267\n",
            "weighted avg       0.79      0.79      0.79      9267\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7930290277328154"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_tester(X_train, X_test, y_train, y_test):\n",
        "  rfc()\n",
        "  return"
      ],
      "metadata": {
        "id": "wZ6STt345x0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #pca\n",
        "# from sklearn.decomposition import pca\n",
        "# pca = PCA(n_composition = 2)\n"
      ],
      "metadata": {
        "id": "1iYyA78yyEPs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}